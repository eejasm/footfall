{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model selection\n",
    "\n",
    "Cross-validation is used here to select the best model. In this script it is used to test the best buffer size to draw around the sensors from within which to draw the environment variables. \n",
    "\n",
    "Tests the performance of a <u>Random Forest Regressor</u>\n",
    "\n",
    "<u> The following variables are included in the model:</u>\n",
    "* Weather variables (rain, temperature, windspeed)\n",
    "* Time variables (Day of week, month, year, time of day, public holiday)\n",
    "* Sensor environment variables:\n",
    "    * Betweenness of the street \n",
    "    * Buildings in proximity to the sensor\n",
    "    * Landmarks in proximity to the sensor  \n",
    "    * Furniture in proximity to the sensor    \n",
    "    * Lights in proximity to the sensor   \n",
    "\n",
    "\n",
    "<u> Model performance is evaluated for a range of buffer sizes around the sensors within which the environment variables are counted</u>:\n",
    "   * 50\n",
    "   * 100\n",
    "   * 200\n",
    "   * 400\n",
    "   * 500\n",
    "   * 600\n",
    "   * 1000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import KFold\n",
    "import numpy as np\n",
    "from sklearn.ensemble import RandomForestRegressor, ExtraTreesRegressor \n",
    "import xgboost as xgb\n",
    "from sklearn.pipeline import Pipeline\n",
    "import folium\n",
    "import branca.colormap as cm\n",
    "from eli5.sklearn import PermutationImportance\n",
    "import joblib\n",
    "import os\n",
    "import psutil\n",
    "\n",
    "from Functions import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run models with cross-validation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define the error metrics for the cross-validation to return, and the parameters of the cross validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "error_metrics = ['neg_mean_absolute_error', 'r2', 'neg_root_mean_squared_error', 'neg_mean_absolute_percentage_error']\n",
    "cv_parameters = KFold(n_splits=10, random_state=1, shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Use CV to return error metrics for the datasets produced with different buffer sizes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Dataframe to store the scores for all the models\n",
    "# error_metric_scores = pd.DataFrame()\n",
    "\n",
    "# # Set up model pipeline\n",
    "# model = Pipeline(steps=[['scaler',StandardScaler()],['rf_regressor', RandomForestRegressor(random_state = 1, n_jobs = 10)]])\n",
    "\n",
    "# # Define parameters\n",
    "# model_name = 'rf_regressor'\n",
    "\n",
    "# # Loop through each buffer size option\n",
    "# for buffer_size_m in [50,100,200,400,500,600,1000]:\n",
    "#     # Prepare the input data\n",
    "#     Xfull, Yfull = prepare_x_y_data(buffer_size_m)\n",
    "    \n",
    "#     # Use cross_validate to return the error scores associated with this model and this data\n",
    "#     start = time()\n",
    "#     model_output = cross_validate(rf_model_pipeline, Xfull, Yfull, cv=cv_parameters, scoring=error_metrics, error_score=\"raise\")\n",
    "#     end = time()\n",
    "#     print('Ran in {} minutes'.format(round((end - start)/60),2))\n",
    "    \n",
    "#     # Formulate the different error scores into a dataframe\n",
    "#     error_metrics_df =pd.DataFrame({'mae': round(abs(model_output['test_neg_mean_absolute_error'].mean()),2), \n",
    "#                   'mape': round(abs(model_output['test_neg_mean_absolute_percentage_error'].mean()),2),\n",
    "#                   'r2': round(abs(model_output['test_r2'].mean()),2), \n",
    "#                   'rmse': round(abs(model_output['test_neg_root_mean_squared_error'].mean()),2)},\n",
    "#                  index =[\"{}\".format(buffer_size_m)])\n",
    "        \n",
    "#     # Add evaluation metric scores for this model to the dataframe containing the metrics for each model\n",
    "#     error_metric_scores = error_metric_scores.append(error_metrics_df)\n",
    "#     # Save error scores for this distance to file\n",
    "#     error_metrics_df.to_csv('PickleFiles/CV/{}/{}_error_metrics_{}m.csv'.format(buffer_size_m, model_name,buffer_size_m),index=False)    \n",
    "\n",
    "# # Save dataframes of error metrics for each buffer distance \n",
    "# error_metric_scores.to_csv('error_metric_scores.csv')   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ran in 185 minutes\n"
     ]
    }
   ],
   "source": [
    "# Dataframe to store the scores for all the models\n",
    "error_metric_scores = pd.DataFrame()\n",
    "\n",
    "# Set up model pipeline\n",
    "model_pipeline = Pipeline(steps=[['scaler',StandardScaler()],['rf_regressor', RandomForestRegressor(random_state = 1, n_jobs = 10)]])\n",
    "\n",
    "# Define parameters\n",
    "model_name = 'rf_regressor'\n",
    "\n",
    "# Loop through each buffer size option\n",
    "for buffer_size_m in [500]:\n",
    "    # Prepare the input data\n",
    "    Xfull, Yfull, data_time_columns = prepare_x_y_data(buffer_size_m)\n",
    "    \n",
    "    # Use cross_validate to return the error scores associated with this model and this data\n",
    "    start = time()\n",
    "    model_output = cross_validate(model_pipeline, Xfull, Yfull, cv=cv_parameters, scoring=error_metrics, error_score=\"raise\")\n",
    "    end = time()\n",
    "    print('Ran in {} minutes'.format(round((end - start)/60),2))\n",
    "    \n",
    "    # Formulate the different error scores into a dataframe\n",
    "    error_metrics_df =pd.DataFrame({'mae': round(abs(model_output['test_neg_mean_absolute_error'].mean()),2), \n",
    "                  'mape': round(abs(model_output['test_neg_mean_absolute_percentage_error'].mean()),2),\n",
    "                  'r2': round(abs(model_output['test_r2'].mean()),2), \n",
    "                  'rmse': round(abs(model_output['test_neg_root_mean_squared_error'].mean()),2)},\n",
    "                 index =[\"{}\".format(buffer_size_m)])\n",
    "        \n",
    "    # Add evaluation metric scores for this model to the dataframe containing the metrics for each model\n",
    "    error_metric_scores = error_metric_scores.append(error_metrics_df)\n",
    "    # Save error scores for this distance to file\n",
    "    #error_metrics_df.to_csv('PickleFiles/CV/{}/{}_error_metrics_{}m.csv'.format(buffer_size_m, model_name,buffer_size_m),index=False)    \n",
    "\n",
    "# Save dataframes of error metrics for each buffer distance \n",
    "#error_metric_scores.to_csv('error_metric_scores.csv')   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>mae</th>\n",
       "      <th>mape</th>\n",
       "      <th>r2</th>\n",
       "      <th>rmse</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>500</th>\n",
       "      <td>75.64</td>\n",
       "      <td>2.108923e+15</td>\n",
       "      <td>0.91</td>\n",
       "      <td>149.93</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       mae          mape    r2    rmse\n",
       "500  75.64  2.108923e+15  0.91  149.93"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "error_metric_scores"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Print table showing error metrics associated with each buffer size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>mae</th>\n",
       "      <th>map</th>\n",
       "      <th>r2</th>\n",
       "      <th>rmse</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>50</th>\n",
       "      <td>61.34</td>\n",
       "      <td>1.493958e+15</td>\n",
       "      <td>0.93</td>\n",
       "      <td>127.24</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>100</th>\n",
       "      <td>60.79</td>\n",
       "      <td>1.508953e+15</td>\n",
       "      <td>0.93</td>\n",
       "      <td>126.02</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>200</th>\n",
       "      <td>60.27</td>\n",
       "      <td>1.513131e+15</td>\n",
       "      <td>0.94</td>\n",
       "      <td>124.99</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>400</th>\n",
       "      <td>60.12</td>\n",
       "      <td>1.521340e+15</td>\n",
       "      <td>0.94</td>\n",
       "      <td>125.01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>500</th>\n",
       "      <td>59.55</td>\n",
       "      <td>1.513783e+15</td>\n",
       "      <td>0.94</td>\n",
       "      <td>123.22</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>600</th>\n",
       "      <td>59.77</td>\n",
       "      <td>1.508504e+15</td>\n",
       "      <td>0.94</td>\n",
       "      <td>123.83</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1000</th>\n",
       "      <td>61.05</td>\n",
       "      <td>1.474851e+15</td>\n",
       "      <td>0.93</td>\n",
       "      <td>126.53</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        mae           map    r2    rmse\n",
       "50    61.34  1.493958e+15  0.93  127.24\n",
       "100   60.79  1.508953e+15  0.93  126.02\n",
       "200   60.27  1.513131e+15  0.94  124.99\n",
       "400   60.12  1.521340e+15  0.94  125.01\n",
       "500   59.55  1.513783e+15  0.94  123.22\n",
       "600   59.77  1.508504e+15  0.94  123.83\n",
       "1000  61.05  1.474851e+15  0.93  126.53"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "error_metric_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_x_y_data(buffer_size_m):\n",
    "    # Read in formatted data\n",
    "    data = pd.read_csv(\"../Cleaned_data/FormattedDataForModelling/formatted_data_for_modelling_allsensors_{}_withsincos.csv\".format(buffer_size_m), \n",
    "                       index_col = False)\n",
    "    data = data.fillna(0)\n",
    "    \n",
    "    ### Delete unneeded columns - we currently include data from all sensors (even incomplete ones)\n",
    "    sensor_ids = data['sensor_id']\n",
    "    data = data.drop(['sensor_id'],axis=1) # don't want this included\n",
    "    # Get rid of columns in which none of the sensors have a value\n",
    "    for column in data.columns:\n",
    "        if np.nanmax(data[column]) ==0:\n",
    "            del data[column]\n",
    "            \n",
    "    # Filter columns using the regex pattern in function input\n",
    "    regex_pattern = 'buildings$|furniture$|landmarks$'\n",
    "    data = data[data.columns.drop(list(data.filter(regex=regex_pattern)))].copy()\n",
    "    \n",
    "    ### Add a random variable (to compare performance of other variables against)\n",
    "    rng = np.random.RandomState(seed=42)\n",
    "    data['random'] = np.random.random(size=len(data))\n",
    "    data[\"random_cat\"] = rng.randint(3, size=data.shape[0])\n",
    "    \n",
    "    ## Prepare data for modelling \n",
    "    ### Split into predictor/predictand variables\n",
    "    Xfull = data.drop(['hourly_counts'], axis =1)\n",
    "    Yfull = data['hourly_counts'].values\n",
    "       \n",
    "    ### Store the (non Sin/Cos) time columns and then remove them (Need them later to segment the results by hour of the day)\n",
    "    data_time_columns = Xfull[['day_of_month_num', 'time', 'weekday_num', 'time_of_day']]\n",
    "    Xfull = Xfull.drop(['day_of_month_num', 'time', 'weekday_num', 'time_of_day','year','datetime', 'month_num'],axis=1)\n",
    "    return Xfull, Yfull, data_time_columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ran in 172 minutes\n"
     ]
    }
   ],
   "source": [
    "Xfull, Yfull, data_time_columns = prepare_x_y_data(buffer_size_m)\n",
    "Xfull.drop(['Sin_month_num', 'Cos_month_num', 'Monday', 'Tuesday', 'Wednesday', 'Thursday', 'Saturday', 'Sunday'], axis=1, inplace =True)\n",
    "\n",
    "# Use cross_validate to return the error scores associated with this model and this data\n",
    "start = time()\n",
    "model_output = cross_validate(model_pipeline, Xfull, Yfull, cv=cv_parameters, scoring=error_metrics, error_score=\"raise\")\n",
    "end = time()\n",
    "print('Ran in {} minutes'.format(round((end - start)/60),2))\n",
    "\n",
    "# Formulate the different error scores into a dataframe\n",
    "error_metrics_df =pd.DataFrame({'mae': round(abs(model_output['test_neg_mean_absolute_error'].mean()),2), \n",
    "              'mape': round(abs(model_output['test_neg_mean_absolute_percentage_error'].mean()),2),\n",
    "              'r2': round(abs(model_output['test_r2'].mean()),2), \n",
    "              'rmse': round(abs(model_output['test_neg_root_mean_squared_error'].mean()),2)},\n",
    "             index =[\"{}\".format(buffer_size_m)])\n",
    "\n",
    "# Add evaluation metric scores for this model to the dataframe containing the metrics for each model\n",
    "error_metric_scores = error_metric_scores.append(error_metrics_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Xfull, Yfull, data_time_columns = prepare_x_y_data(buffer_size_m)\n",
    "Xfull.drop(['Sin_weekday_num', 'Cos_weekday_num', 'month_2', 'month_3', 'month_4', 'month_5', 'month_6', 'month_7',\n",
    "           \"month_8\", \"month_9\", \"month_10\", \"month_11\", \"month_12\"], axis=1, inplace =True)\n",
    "Xfull.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ran in 166 minutes\n"
     ]
    }
   ],
   "source": [
    "# Use cross_validate to return the error scores associated with this model and this data\n",
    "start = time()\n",
    "model_output = cross_validate(model_pipeline, Xfull, Yfull, cv=cv_parameters, scoring=error_metrics, error_score=\"raise\")\n",
    "end = time()\n",
    "print('Ran in {} minutes'.format(round((end - start)/60),2))\n",
    "\n",
    "# Formulate the different error scores into a dataframe\n",
    "error_metrics_df =pd.DataFrame({'mae': round(abs(model_output['test_neg_mean_absolute_error'].mean()),2), \n",
    "              'mape': round(abs(model_output['test_neg_mean_absolute_percentage_error'].mean()),2),\n",
    "              'r2': round(abs(model_output['test_r2'].mean()),2), \n",
    "              'rmse': round(abs(model_output['test_neg_root_mean_squared_error'].mean()),2)},\n",
    "             index =[\"{}_sin_wd_nm, months\".format(buffer_size_m)])\n",
    "\n",
    "# Add evaluation metric scores for this model to the dataframe containing the metrics for each model\n",
    "error_metric_scores = error_metric_scores.append(error_metrics_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>mae</th>\n",
       "      <th>mape</th>\n",
       "      <th>r2</th>\n",
       "      <th>rmse</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>500</th>\n",
       "      <td>75.64</td>\n",
       "      <td>2.108923e+15</td>\n",
       "      <td>0.91</td>\n",
       "      <td>149.93</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>500</th>\n",
       "      <td>74.29</td>\n",
       "      <td>2.077179e+15</td>\n",
       "      <td>0.91</td>\n",
       "      <td>147.42</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>500_sin_wd_nm, months</th>\n",
       "      <td>74.44</td>\n",
       "      <td>2.089302e+15</td>\n",
       "      <td>0.91</td>\n",
       "      <td>147.96</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                         mae          mape    r2    rmse\n",
       "500                    75.64  2.108923e+15  0.91  149.93\n",
       "500                    74.29  2.077179e+15  0.91  147.42\n",
       "500_sin_wd_nm, months  74.44  2.089302e+15  0.91  147.96"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "error_metric_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "Xfull, Yfull, data_time_columns = prepare_x_y_data(buffer_size_m)\n",
    "Xfull.drop([ 'Monday', 'Tuesday', 'Wednesday', 'Thursday', 'Saturday', 'Sunday', 'month_2', 'month_3', 'month_4', 'month_5', 'month_6', 'month_7',\n",
    "           \"month_8\", \"month_9\", \"month_10\", \"month_11\", \"month_12\"], axis=1, inplace =True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ran in 162 minutes\n"
     ]
    }
   ],
   "source": [
    "# Use cross_validate to return the error scores associated with this model and this data\n",
    "start = time()\n",
    "model_output = cross_validate(model_pipeline, Xfull, Yfull, cv=cv_parameters, scoring=error_metrics, error_score=\"raise\")\n",
    "end = time()\n",
    "print('Ran in {} minutes'.format(round((end - start)/60),2))\n",
    "\n",
    "# Formulate the different error scores into a dataframe\n",
    "error_metrics_df =pd.DataFrame({'mae': round(abs(model_output['test_neg_mean_absolute_error'].mean()),2), \n",
    "              'mape': round(abs(model_output['test_neg_mean_absolute_percentage_error'].mean()),2),\n",
    "              'r2': round(abs(model_output['test_r2'].mean()),2), \n",
    "              'rmse': round(abs(model_output['test_neg_root_mean_squared_error'].mean()),2)},\n",
    "             index =[\"{}_sin_wd_nm, months\".format(buffer_size_m)])\n",
    "\n",
    "# Add evaluation metric scores for this model to the dataframe containing the metrics for each model\n",
    "error_metric_scores = error_metric_scores.append(error_metrics_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>mae</th>\n",
       "      <th>mape</th>\n",
       "      <th>r2</th>\n",
       "      <th>rmse</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>500</th>\n",
       "      <td>75.64</td>\n",
       "      <td>2.108923e+15</td>\n",
       "      <td>0.91</td>\n",
       "      <td>149.93</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>500</th>\n",
       "      <td>74.29</td>\n",
       "      <td>2.077179e+15</td>\n",
       "      <td>0.91</td>\n",
       "      <td>147.42</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>500_sin_wd_nm, months</th>\n",
       "      <td>74.44</td>\n",
       "      <td>2.089302e+15</td>\n",
       "      <td>0.91</td>\n",
       "      <td>147.96</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>500_sin_wd_nm, months</th>\n",
       "      <td>73.03</td>\n",
       "      <td>2.068655e+15</td>\n",
       "      <td>0.91</td>\n",
       "      <td>145.32</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                         mae          mape    r2    rmse\n",
       "500                    75.64  2.108923e+15  0.91  149.93\n",
       "500                    74.29  2.077179e+15  0.91  147.42\n",
       "500_sin_wd_nm, months  74.44  2.089302e+15  0.91  147.96\n",
       "500_sin_wd_nm, months  73.03  2.068655e+15  0.91  145.32"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "error_metric_scores"
   ]
  }
 ],
 "metadata": {
  "hide_input": false,
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
