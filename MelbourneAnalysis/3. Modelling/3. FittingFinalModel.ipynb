{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Modelling\n",
    "<u>Tests using the following models :</u>\n",
    "* Linear regression\n",
    "* Random forest regressor\n",
    "* Ridge and Lasso Regularization (add on to linear modelling?)\n",
    "\n",
    "<u> Tests using the following variables:</u>\n",
    "* Weather variables (rain, temperature, windspeed)\n",
    "* Time variables (Day of week, month, year, time of day, public holiday)\n",
    "* Sensor environment variables:\n",
    "    * Sensor_id\n",
    "    * Betweenness of the street \n",
    "    * Buildings in proximity to the sensor\n",
    "    * Landmarks in proximity to the sensor  \n",
    "    * Furniture in proximity to the sensor    \n",
    "    * Lights in proximity to the sensor   \n",
    "\n",
    "\n",
    "Normalise variables: should this be with MinMax or StandardScaler??\n",
    "\n",
    "\n",
    "Process:\n",
    "* Keep only data from sensor's with relatively complete data\n",
    "* Split data into training ( 75%) and test (25%)\n",
    "* Define the models to use in testing (linear regression, random forest, xgboost)\n",
    "* Define the error metrics to use in evaluating the model performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import copy\n",
    "import pandas as pd\n",
    "from sklearn.linear_model import LinearRegression, Ridge, Lasso, ElasticNet\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split, cross_val_score, GridSearchCV\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import classification_report, mean_squared_error,r2_score, accuracy_score, mean_absolute_error, mean_absolute_percentage_error\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.ensemble import RandomForestRegressor, ExtraTreesRegressor\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "import time as thetime\n",
    "from sklearn.model_selection import cross_validate\n",
    "from xgboost import XGBClassifier, XGBRegressor\n",
    "from time import time\n",
    "from sklearn.inspection import permutation_importance\n",
    "from scipy import stats\n",
    "import math\n",
    "\n",
    "from eli5.sklearn import PermutationImportance\n",
    "from sklearn.model_selection import cross_val_predict\n",
    "\n",
    "from warnings import simplefilter\n",
    "simplefilter(action='ignore', category=FutureWarning)\n",
    "\n",
    "import multiprocessing\n",
    "\n",
    "# To display tables in HTML output\n",
    "from IPython.display import HTML, display\n",
    "\n",
    "from Functions import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Read in formatted data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv(\"../Cleaned_data/formatted_data_for_modelling_allsensors.csv\", index_col = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Delete unneeded columns\n",
    "We currently include data from all sensors (even incomplete ones)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = data.drop(['sensor_id'],axis=1) # don't want this included\n",
    "# Get rid of columns in which none of the sensors have a value\n",
    "for column in data.columns:\n",
    "    if np.nanmax(data[column]) ==0:\n",
    "        del data[column]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter columns using the regex pattern in function input\n",
    "regex_pattern = 'buildings$|furniture$|landmarks$'\n",
    "data = data[data.columns.drop(list(data.filter(regex=regex_pattern)))].copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Add a random variable (to compare performance of other variables against)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "rng = np.random.RandomState(seed=42)\n",
    "data['random'] = np.random.random(size=len(data))\n",
    "data[\"random_cat\"] = rng.randint(3, size=data.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(len(data['random'].unique()))\n",
    "# print(len(data))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare data for modelling \n",
    "### Split into predictor/predictand variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The predictor variables\n",
    "Xfull = data.drop(['hourly_counts'], axis =1)\n",
    "# The variable to be predicted\n",
    "Yfull = data['hourly_counts'].values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Store the (non Sin/Cos) time columns and then remove them\n",
    "Need them later to segment the results by hour of the day"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_time_columns = Xfull[['day_of_month_num', 'time', 'weekday_num', 'time_of_day']]\n",
    "Xfull = Xfull.drop(['day_of_month_num', 'time', 'weekday_num', 'time_of_day','year', 'month','day', 'datetime', 'month_num'],axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Random Forest was the best performing model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fit the final model\n",
    "For this, we use all of the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['rf_model_pipeline2.sav']"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 1\n",
    "rf_model_pipeline2 = Pipeline(steps=[['scaler',StandardScaler()],\n",
    "                                    ['rf_regressor',RandomForestRegressor(random_state = 1, n_jobs = 32)]])\n",
    "rf_model_pipeline2.fit(Xfull[:10], Yfull[:10])\n",
    "# 2\n",
    "# rf_model_pipeline2 = Pipeline(steps=[['scaler',StandardScaler()],\n",
    "#                                     ['rf_regressor',RandomForestRegressor(n_estimators = 500, random_state = 2, n_jobs = 32)]])\n",
    "# rf_model_pipeline2.fit(Xfull, Yfull);\n",
    "# # 3\n",
    "# rf_model_pipeline3 = Pipeline(steps=[['scaler',StandardScaler()],\n",
    "#                                     ['rf_regressor',RandomForestRegressor(n_estimators = 500, random_state = 3, n_jobs = 32)]])\n",
    "# rf_model_pipeline3.fit(Xfull, Yfull);"
   ]
  }
 ],
 "metadata": {
  "hide_input": false,
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
