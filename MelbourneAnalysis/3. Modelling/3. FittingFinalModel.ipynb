{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Modelling\n",
    "<u>Tests using the following models :</u>\n",
    "* Linear regression\n",
    "* Random forest regressor\n",
    "* Ridge and Lasso Regularization (add on to linear modelling?)\n",
    "\n",
    "<u> Tests using the following variables:</u>\n",
    "* Weather variables (rain, temperature, windspeed)\n",
    "* Time variables (Day of week, month, year, time of day, public holiday)\n",
    "* Sensor environment variables:\n",
    "    * Sensor_id\n",
    "    * Betweenness of the street \n",
    "    * Buildings in proximity to the sensor\n",
    "    * Landmarks in proximity to the sensor  \n",
    "    * Furniture in proximity to the sensor    \n",
    "    * Lights in proximity to the sensor   \n",
    "\n",
    "\n",
    "Normalise variables: should this be with MinMax or StandardScaler??\n",
    "\n",
    "\n",
    "Process:\n",
    "* Keep only data from sensor's with relatively complete data\n",
    "* Split data into training ( 75%) and test (25%)\n",
    "* Define the models to use in testing (linear regression, random forest, xgboost)\n",
    "* Define the error metrics to use in evaluating the model performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import copy\n",
    "import pandas as pd\n",
    "from sklearn.linear_model import LinearRegression, Ridge, Lasso, ElasticNet\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split, cross_val_score, GridSearchCV\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import classification_report, mean_squared_error,r2_score, accuracy_score, mean_absolute_error, mean_absolute_percentage_error\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.ensemble import RandomForestRegressor, ExtraTreesRegressor\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "import time as thetime\n",
    "from sklearn.model_selection import cross_validate\n",
    "from xgboost import XGBClassifier, XGBRegressor\n",
    "from time import time\n",
    "from sklearn.inspection import permutation_importance\n",
    "from scipy import stats\n",
    "\n",
    "from eli5.sklearn import PermutationImportance\n",
    "from sklearn.model_selection import cross_val_predict\n",
    "\n",
    "from warnings import simplefilter\n",
    "simplefilter(action='ignore', category=FutureWarning)\n",
    "\n",
    "from Functions import *\n",
    "import joblib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "buffer_size_m = 50"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Read in formatted data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv(\"../Cleaned_data/FormattedDataForModelling/formatted_data_for_modelling_allsensors_{}.csv\".format(buffer_size_m), index_col = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Delete unneeded columns\n",
    "We currently include data from all sensors (even incomplete ones)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = data.drop(['sensor_id'],axis=1) # don't want this included\n",
    "# Get rid of columns in which none of the sensors have a value\n",
    "for column in data.columns:\n",
    "    if np.nanmax(data[column]) ==0:\n",
    "        del data[column]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter columns using the regex pattern in function input\n",
    "regex_pattern = 'buildings$|furniture$|landmarks$'\n",
    "data = data[data.columns.drop(list(data.filter(regex=regex_pattern)))].copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Add a random variable (to compare performance of other variables against)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "rng = np.random.RandomState(seed=42)\n",
    "data['random'] = np.random.random(size=len(data))\n",
    "data[\"random_cat\"] = rng.randint(3, size=data.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(len(data['random'].unique()))\n",
    "# print(len(data))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Date based variables: Option 1 - Use the Cos/sin cyclical variable versions\n",
    "Store the (non Sin/Cos) time columns and then remove them\n",
    "Need them later to segment the results by hour of the day"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data_time_columns = data[['day_of_month_num', 'time', 'weekday_num', 'time_of_day']]\n",
    "# data = data.drop(['day_of_month_num', 'time', 'weekday_num', 'time_of_day','year', 'month','day', 'datetime', 'month_num'],axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Date based variables: Option 2 - Create Dummy Variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "for date_col in ['day', 'month',]:\n",
    "    date_col_dummy =  pd.get_dummies(data[date_col], drop_first = True)\n",
    "    if date_col =='month':\n",
    "        date_col_dummy.columns= prepend(date_col_dummy.columns.values, 'month_')\n",
    "#     if date_col =='year':\n",
    "#         date_col_dummy.columns= prepend(date_col_dummy.columns.values, 'year_')\n",
    "    data = pd.concat([data, date_col_dummy],axis=1)\n",
    "    del data[date_col]\n",
    "# data_time_columns = data[['time']]\n",
    "data = data.drop(['datetime', 'time_of_day',  'time', 'weekday_num','month_num','Sin_month_num', 'Cos_month_num',\n",
    "       'Sin_weekday_num', 'Cos_weekday_num', \"day_of_month_num\" ],axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['year_normalised']=data['year']-2010\n",
    "del data[\"year\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare data for modelling \n",
    "### Split into predictor/predictand variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The predictor variables\n",
    "Xfull = data.drop(['hourly_counts'], axis =1)\n",
    "# The variable to be predicted\n",
    "Yfull = data['hourly_counts'].values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fit the final model\n",
    "Random Forest was the best performing model from CV  \n",
    "For this, we use all of the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Fit the final model --- 1\n",
    "print(\"fitting model 1\")\n",
    "rf_model_pipeline1 = Pipeline(steps=[['scaler',StandardScaler()],\n",
    "                                    ['rf_regressor',RandomForestRegressor(random_state = 1, n_jobs = 32)]])\n",
    "rf_model_pipeline1.fit(Xfull, Yfull)\n",
    "print(\"saving pickled file\")\n",
    "# Save to pickled file\n",
    "filename = 'PickleFiles/FinalModels/rf_model_pipeline1_combined_features_{}.pkl'.format(buffer_size_m)\n",
    "joblib.dump(rf_model_pipeline1, filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "Xfull.to_csv('PickleFiles/FinalModels/Xfull_rf_model_pipeline1_combined_features_{}.csv'.format(buffer_size_m), index=False)\n",
    "Yfull_df=pd.DataFrame(Yfull)\n",
    "Yfull_df.to_csv('PickleFiles/FinalModels/Yfull_rf_model_pipeline1_combined_features_{}.csv'.format(buffer_size_m), index=False)"
   ]
  }
 ],
 "metadata": {
  "hide_input": false,
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
