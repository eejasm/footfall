{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Modelling\n",
    "<u>Tests using the following models :</u>\n",
    "* Linear regression\n",
    "* Random forest regressor\n",
    "* Ridge and Lasso Regularization (add on to linear modelling?)\n",
    "\n",
    "<u> Tests using the following variables:</u>\n",
    "* Weather variables (rain, temperature, windspeed)\n",
    "* Time variables (Day of week, month, year, time of day, public holiday)\n",
    "* Sensor environment variables:\n",
    "    * Sensor_id\n",
    "    * Betweenness of the street \n",
    "    * Buildings in proximity to the sensor\n",
    "    * Landmarks in proximity to the sensor  \n",
    "    * Furniture in proximity to the sensor    \n",
    "    * Lights in proximity to the sensor   \n",
    "\n",
    "\n",
    "Normalise variables: should this be with MinMax or StandardScaler??\n",
    "\n",
    "\n",
    "Process:\n",
    "* Keep only data from sensor's with relatively complete data\n",
    "* Split data into training ( 75%) and test (25%)\n",
    "* Define the models to use in testing (linear regression, random forest, xgboost)\n",
    "* Define the error metrics to use in evaluating the model performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import copy\n",
    "import pandas as pd\n",
    "# from sklearn.linear_model import LinearRegression, Ridge, Lasso, ElasticNet\n",
    "# from sklearn.preprocessing import StandardScaler\n",
    "# from sklearn.model_selection import train_test_split, cross_val_score, GridSearchCV\n",
    "# import numpy as np\n",
    "# import matplotlib.pyplot as plt\n",
    "# from sklearn.metrics import classification_report, mean_squared_error,r2_score, accuracy_score, mean_absolute_error, mean_absolute_percentage_error\n",
    "# from sklearn.tree import DecisionTreeRegressor\n",
    "# from sklearn.ensemble import RandomForestRegressor, ExtraTreesRegressor\n",
    "# from sklearn.preprocessing import MinMaxScaler\n",
    "# import time as thetime\n",
    "# from sklearn.model_selection import cross_validate\n",
    "# from xgboost import XGBClassifier, XGBRegressor\n",
    "# from time import time\n",
    "# from sklearn.inspection import permutation_importance\n",
    "# from scipy import stats\n",
    "# import math\n",
    "\n",
    "from eli5.sklearn import PermutationImportance\n",
    "# from sklearn.model_selection import cross_val_predict\n",
    "\n",
    "# from warnings import simplefilter\n",
    "# simplefilter(action='ignore', category=FutureWarning)\n",
    "import joblib \n",
    "# import multiprocessing\n",
    "\n",
    "# To display tables in HTML output\n",
    "from IPython.display import HTML, display\n",
    "\n",
    "from Functions import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Read in formatted data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv(\"../Cleaned_data/formatted_data_for_modelling_allsensors_combined_features.csv\", index_col = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Delete unneeded columns\n",
    "We currently include data from all sensors (even incomplete ones)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = data.drop(['sensor_id'],axis=1) # don't want this included\n",
    "# Get rid of columns in which none of the sensors have a value\n",
    "for column in data.columns:\n",
    "    if np.nanmax(data[column]) ==0:\n",
    "        del data[column]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter columns using the regex pattern in function input\n",
    "regex_pattern = 'buildings$|furniture$|landmarks$'\n",
    "data = data[data.columns.drop(list(data.filter(regex=regex_pattern)))].copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Add a random variable (to compare performance of other variables against)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "rng = np.random.RandomState(seed=42)\n",
    "data['random'] = np.random.random(size=len(data))\n",
    "data[\"random_cat\"] = rng.randint(3, size=data.shape[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare data for modelling \n",
    "### Split into predictor/predictand variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The predictor variables\n",
    "Xfull = data.drop(['hourly_counts'], axis =1)\n",
    "# The variable to be predicted\n",
    "Yfull = data['hourly_counts'].values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Store the (non Sin/Cos) time columns and then remove them\n",
    "Need them later to segment the results by hour of the day"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_time_columns = Xfull[['day_of_month_num', 'time', 'weekday_num', 'time_of_day']]\n",
    "Xfull = Xfull.drop(['day_of_month_num', 'time', 'weekday_num', 'time_of_day','year', 'month','day', 'datetime', 'month_num'],axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Random Forest was the best performing model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load the fitted model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "rf_model_pipeline1 = joblib.load('PickleFiles/rf_model_pipeline1_combined_features.fit.sav')\n",
    "rf_model_pipeline2 = joblib.load('PickleFiles/rf_model_pipeline2_combined_features.fit.sav')\n",
    "rf_model_pipeline3 = joblib.load('PickleFiles/rf_model_pipeline3_combined_features.fit.sav')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Find Gini impurity feature importances\n",
    "Gini feature importance (or mean decrease in impurity) counts the number of times a feature is used to split a node, weighted by the number of samples it splits."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['Temp', 'Humidity', 'Pressure', 'Rain', 'WindSpeed', 'public_holiday',\n",
       "       'school_holiday', 'betweenness', 'lights', 'furniture_Bicycle Rails',\n",
       "       'furniture_Bollard', 'furniture_Floral Crate/Planter Box',\n",
       "       'furniture_Litter Bin', 'landmarks_Community Use',\n",
       "       'furniture_Tree Guard', 'landmarks_Place of Worship',\n",
       "       'landmarks_Mixed Use', 'furniture_Picnic Setting',\n",
       "       'landmarks_Leisure/Recreation', 'landmarks_Office',\n",
       "       'furniture_Barbeque', 'landmarks_Place Of Assembly', 'furniture_Hoop',\n",
       "       'furniture_Horse Trough', 'landmarks_Health Services', 'avg_n_floors',\n",
       "       'buildings_Community Use', 'buildings_Entertainment',\n",
       "       'buildings_Office', 'buildings_Residential', 'buildings_Unoccupied',\n",
       "       'buildings_Working', 'buildings_Events', 'buildings_Hospital/Clinic',\n",
       "       'buildings_Public Display Area', 'buildings_Parking',\n",
       "       'buildings_Education', 'Sin_time', 'Cos_time', 'Sin_month_num',\n",
       "       'Cos_month_num', 'Sin_weekday_num', 'Cos_weekday_num',\n",
       "       'transport_feature',\n",
       "       'retail_landmark_and_storageBuilding_and_seat_drinkingfountain_infoPillar',\n",
       "       'random', 'random_cat'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = rf_model_pipeline1['rf_regressor']\n",
    "rf_importances = list(model.feature_importances_)\n",
    "len(rf_importances)\n",
    "Xfull.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "All arrays must be of the same length",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_221315/4077442668.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      8\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mrf_feature_importances\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m \u001b[0mgi_1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfind_gini_importance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrf_model_pipeline1\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'rf_regressor'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     11\u001b[0m \u001b[0mgi_2\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfind_gini_importance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrf_model_pipeline2\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'rf_regressor'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0mgi_3\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfind_gini_importance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrf_model_pipeline3\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'rf_regressor'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/tmp/ipykernel_221315/4077442668.py\u001b[0m in \u001b[0;36mfind_gini_importance\u001b[0;34m(model)\u001b[0m\n\u001b[1;32m      2\u001b[0m     \u001b[0;31m# Get numerical feature importances\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m     \u001b[0mrf_importances\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfeature_importances_\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m     \u001b[0mrf_feature_importances\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDataFrame\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0;34m'feature'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mXfull\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'importance'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mrf_importances\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m     \u001b[0mrf_feature_importances\u001b[0m\u001b[0;34m=\u001b[0m \u001b[0mrf_feature_importances\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msort_values\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mby\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'importance'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mascending\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m     \u001b[0;31m# Get just the features that scored more highly than a random feature\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/nfs/a321/gy17m2a/anaconda_install/anaconda3/envs/footfall_ml/lib/python3.7/site-packages/pandas/core/frame.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, data, index, columns, dtype, copy)\u001b[0m\n\u001b[1;32m    612\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdict\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    613\u001b[0m             \u001b[0;31m# GH#38939 de facto copy defaults to False only in non-dict cases\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 614\u001b[0;31m             \u001b[0mmgr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdict_to_mgr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindex\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcolumns\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcopy\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcopy\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtyp\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmanager\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    615\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mma\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mMaskedArray\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    616\u001b[0m             \u001b[0;32mimport\u001b[0m \u001b[0mnumpy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mma\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmrecords\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mmrecords\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/nfs/a321/gy17m2a/anaconda_install/anaconda3/envs/footfall_ml/lib/python3.7/site-packages/pandas/core/internals/construction.py\u001b[0m in \u001b[0;36mdict_to_mgr\u001b[0;34m(data, index, columns, dtype, typ, copy)\u001b[0m\n\u001b[1;32m    463\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    464\u001b[0m     return arrays_to_mgr(\n\u001b[0;32m--> 465\u001b[0;31m         \u001b[0marrays\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata_names\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindex\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcolumns\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtyp\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtyp\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconsolidate\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcopy\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    466\u001b[0m     )\n\u001b[1;32m    467\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/nfs/a321/gy17m2a/anaconda_install/anaconda3/envs/footfall_ml/lib/python3.7/site-packages/pandas/core/internals/construction.py\u001b[0m in \u001b[0;36marrays_to_mgr\u001b[0;34m(arrays, arr_names, index, columns, dtype, verify_integrity, typ, consolidate)\u001b[0m\n\u001b[1;32m    117\u001b[0m         \u001b[0;31m# figure out the index, if necessary\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    118\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mindex\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 119\u001b[0;31m             \u001b[0mindex\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_extract_index\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marrays\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    120\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    121\u001b[0m             \u001b[0mindex\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mensure_index\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/nfs/a321/gy17m2a/anaconda_install/anaconda3/envs/footfall_ml/lib/python3.7/site-packages/pandas/core/internals/construction.py\u001b[0m in \u001b[0;36m_extract_index\u001b[0;34m(data)\u001b[0m\n\u001b[1;32m    633\u001b[0m             \u001b[0mlengths\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mraw_lengths\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    634\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlengths\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 635\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"All arrays must be of the same length\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    636\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    637\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mhave_dicts\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: All arrays must be of the same length"
     ]
    }
   ],
   "source": [
    "def find_gini_importance(model):\n",
    "    # Get numerical feature importances\n",
    "    rf_importances = list(model.feature_importances_)\n",
    "    rf_feature_importances = pd.DataFrame({'feature': Xfull.columns,'importance':rf_importances})      \n",
    "    rf_feature_importances= rf_feature_importances.sort_values(by = 'importance', ascending = True)\n",
    "    # Get just the features that scored more highly than a random feature\n",
    "    rf_feature_importances_overrandom = rf_feature_importances[rf_feature_importances['importance']>rf_feature_importances.query(\"feature=='random'\")[\"importance\"].values[0]]\n",
    "    return rf_feature_importances\n",
    "    \n",
    "gi_1 = find_gini_importance(rf_model_pipeline1['rf_regressor'])    \n",
    "gi_2 = find_gini_importance(rf_model_pipeline2['rf_regressor'])   \n",
    "gi_3 = find_gini_importance(rf_model_pipeline3['rf_regressor'])   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Find the permutation importances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is based on analysis of how the score changes when the feature is not available\n",
    "# Thus we need to chose the accuracy score to use\n",
    "def find_permutation_importance(model, Xfull, Yfull, n_iter):\n",
    "    # instantiate permuter object\n",
    "    permuter = PermutationImportance(model, scoring='neg_mean_absolute_error', cv='prefit', n_iter=n_iter)\n",
    "    permuter.fit(Xfull.values, Yfull)\n",
    "    # Create a dataframe containing the mean results (and std)\n",
    "    pi_meanvalues_df = pd.DataFrame({'feature':Xfull.columns,\n",
    "                  'importance':permuter.feature_importances_,\n",
    "                  'Feature_importance_std': permuter.feature_importances_std_}).sort_values('importance', ascending = True)\n",
    "    # Get the raw results for each permutation, and store as a dataframe\n",
    "    pi_raw_results = permuter.results_  \n",
    "    raw_importances = pd.DataFrame({'feature_list':list(Xfull.columns)})\n",
    "    for num,results in enumerate(permuter.results_):\n",
    "        raw_importances[num] = results\n",
    "    raw_importances =raw_importances.sort_values(by=0, ascending=False)\n",
    "    raw_importances.reset_index(drop = True, inplace=True)\n",
    "    \n",
    "    # Get just the features that scored more highly than a random feature\n",
    "    return pi_meanvalues_df, raw_importances\n",
    "\n",
    "pi_1, pi_1_raw = find_permutation_importance(rf_model_pipeline1['rf_regressor'], Xfull, Yfull, n_iter=3)    \n",
    "pi_2, pi_2_raw = find_permutation_importance(rf_model_pipeline2['rf_regressor'], Xfull, Yfull, n_iter=3)     \n",
    "pi_3, pi_3_raw = find_permutation_importance(rf_model_pipeline3['rf_regressor'], Xfull, Yfull, n_iter=3)  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plot and compare feature importances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# cardinality_df = pd.DataFrame({'feature':Xfull.columns, 'n_features':Xfull.nunique()})\n",
    "# cardinality_df = cardinality_df.merge(pi_1, on='feature', how='outer')\n",
    "# cardinality_df = cardinality_df.drop([0,1,2,4, 45])\n",
    "# plt.scatter(cardinality_df['n_features'], cardinality_df['importance'])\n",
    "# # plt.scatter(cardinality_df[:-2]['n_features'], cardinality_df[:-2]['importance'])\n",
    "# # cardinality_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_compare_importances(axs,gini_importances, perm_importances, above_random_cat = False):\n",
    "    \n",
    "    if above_random_cat == 'random_num':\n",
    "        gini_importances = gini_importances[gini_importances['importance']>gini_importances.query(\"feature=='random'\")[\"importance\"].values[0]]\n",
    "        perm_importances = perm_importances[perm_importances['importance']>perm_importances.query(\"feature=='random'\")[\"importance\"].values[0]]\n",
    "    elif above_random_cat == 'random_cat':\n",
    "        gini_importances = gini_importances[gini_importances['importance']>gini_importances.query(\"feature=='random_cat'\")[\"importance\"].values[0]]\n",
    "        perm_importances = perm_importances[perm_importances['importance']>perm_importances.query(\"feature=='random_cat'\")[\"importance\"].values[0]]\n",
    "        \n",
    "    axs[0].barh(range(len(gini_importances['importance'])), gini_importances[\"importance\"])\n",
    "    axs[0].set_yticks(range(len(gini_importances[\"feature\"])))\n",
    "    _ = axs[0].set_yticklabels(np.array(gini_importances[\"feature\"]))\n",
    "    axs[0].set_title('Gini importance')\n",
    "\n",
    "    axs[1].barh(range(len(perm_importances['importance'])),\n",
    "             perm_importances['importance'],\n",
    "             xerr=perm_importances['Feature_importance_std'])\n",
    "    axs[1].set_yticks(range(len(perm_importances['importance'])))\n",
    "    _ = axs[1].set_yticklabels(perm_importances['feature'])  \n",
    "    axs[1].set_title('Permutation importance')\n",
    "\n",
    "    \n",
    "fig, axs = plt.subplots(ncols = 2, figsize=(15,8))\n",
    "plot_compare_importances(axs, gi_1, pi_1, above_random_cat='random_cat')   \n",
    "\n",
    "fig, axs = plt.subplots(ncols = 2, figsize=(15,8))\n",
    "plot_compare_importances(axs, gi_2, pi_2, above_random_cat='random_cat')   \n",
    "\n",
    "fig, axs = plt.subplots(ncols = 2, figsize=(15,8))\n",
    "plot_compare_importances(axs, gi_3, pi_3, above_random_cat='random_cat')   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plots!?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pi_1_raw"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fig, axs = plt.subplots(nrows=1, ncols=3, constrained_layout=True, figsize = (15,5))\n",
    "\n",
    "# axs[0].barh(rf_feature_importances['feature'][:10], rf_feature_importances['importance'][:10])\n",
    "# axs[0].invert_yaxis()\n",
    "# plt.rcParams.update({'font.size': 5})\n",
    "# axs[0].set_title(\"Feature Importance\", fontsize=15, y=1.01)\n",
    "# axs[0].set_xlabel('Importance', fontsize = 12)\n",
    "# axs[0].set_ylabel('Feature', fontsize = 12)\n",
    "# axs[0].xaxis.set_tick_params(labelsize='xx-large')\n",
    "# axs[0].yaxis.set_tick_params(labelsize='xx-large')\n",
    "\n",
    "# using_datashader(axs[1],Yfull, rf_predictions, 'linear')\n",
    "# axs[1].plot([Y_test.min(), Yfull.max()], [Yfull.min(), Y_test.max()], c='k', lw=0.5)\n",
    "# axs[1].set_ylabel(\"Predicted Values\", size=10)\n",
    "# axs[1].set_xlabel(\"Actual Values\", size=10)\n",
    "# axs[1].xaxis.set_tick_params(labelsize='xx-large')\n",
    "# axs[1].yaxis.set_tick_params(labelsize='xx-large')\n",
    "# axs[1].set_xlim([0, 2000])\n",
    "# axs[1].set_ylim([0, 2000])\n",
    "\n",
    "# using_datashader(axs[2], Y_test, rf_predictions, 'log')\n",
    "# axs[2].plot([Y_test.min(), Y_test.max()], [Y_test.min(), Y_test.max()], c='k', lw=0.5)\n",
    "# axs[2].set_ylabel(\"Predicted Values\", size=10)\n",
    "# axs[2].set_xlabel(\"Actual Values\", size=10)\n",
    "# axs[2].xaxis.set_tick_params(labelsize='xx-large')\n",
    "# axs[2].yaxis.set_tick_params(labelsize='xx-large')\n",
    "# axs[2].set_xlim([0, 2000])\n",
    "# axs[2].set_ylim([0, 2000])\n",
    "\n",
    "# plt.show()"
   ]
  }
 ],
 "metadata": {
  "hide_input": false,
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
