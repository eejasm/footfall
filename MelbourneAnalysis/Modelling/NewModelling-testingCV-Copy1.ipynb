{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Modelling\n",
    "<u>Tests using the following models :</u>\n",
    "* Linear regression\n",
    "* Random forest regressor\n",
    "* Ridge and Lasso Regularization (add on to linear modelling?)\n",
    "\n",
    "<u> Tests using the following variables:</u>\n",
    "* Weather variables (rain, temperature, windspeed)\n",
    "* Time variables (Day of week, month, year, time of day, public holiday)\n",
    "* Sensor environment variables:\n",
    "    * Sensor_id\n",
    "    * Betweenness of the street \n",
    "    * Buildings in proximity to the sensor\n",
    "    * Landmarks in proximity to the sensor  \n",
    "    * Furniture in proximity to the sensor    \n",
    "    * Lights in proximity to the sensor   \n",
    "\n",
    "\n",
    "Normalise variables: should this be with MinMax or StandardScaler??"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import copy\n",
    "import pandas as pd\n",
    "from sklearn.linear_model import LinearRegression, Ridge, Lasso, ElasticNet\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split, cross_val_score, GridSearchCV\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import classification_report, mean_squared_error,r2_score\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "import time as thetime\n",
    "from sklearn.model_selection import cross_validate\n",
    "from xgboost import XGBClassifier, XGBRegressor\n",
    "\n",
    "from warnings import simplefilter\n",
    "simplefilter(action='ignore', category=FutureWarning)\n",
    "\n",
    "import multiprocessing\n",
    "\n",
    "# To display tables in HTML output\n",
    "from IPython.display import HTML, display\n",
    "\n",
    "from Functions import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-20T08:09:42.167895Z",
     "start_time": "2020-05-20T08:09:36.778655Z"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Read in formatted data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv(\"formatted_data_for_modelling.csv\", index_col = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Keep only sensors with relatively complete data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Filter to include just sensors which we know have quite complete data \n",
    "data = data[data['sensor_id'].isin([2,6,9,10,14,18])]\n",
    "data.reset_index(inplace=True, drop = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = data.drop(['Pressure', 'Humidity'],axis=1) # seem obviously irrelevant\n",
    "# data = data[data.columns.drop(list(data.filter(regex='h_')))] # (as these are replaced by the categorical ones)\n",
    "data = data.drop(['sensor_id'],axis=1) # don't want this included\n",
    "# Get rid of columns in which none of the sensors have a value\n",
    "for column in data.columns:\n",
    "    if np.nanmax(data[column]) ==0:\n",
    "        del data[column]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_subtypes = data.drop(['buildings', 'landmarks', 'furniture'],axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare data for modelling - split into predictor/predictand variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The predictor variables\n",
    "Xfull = df_subtypes.drop(['hourly_counts'], axis =1)\n",
    "\n",
    "# The variable to be predicted\n",
    "Yfull = df_subtypes['hourly_counts'].values\n",
    "\n",
    "# Split data into training and test sets\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(Xfull, Yfull, test_size=0.6666, random_state=123)\n",
    "\n",
    "#### Standardize both training and testing data\n",
    "scaler = StandardScaler()\n",
    "X_train = pd.DataFrame(scaler.fit_transform(X_train), columns=X_train.columns)\n",
    "X_test = pd.DataFrame(scaler.transform(X_test), columns=X_test.columns)\n",
    "\n",
    "# \n",
    "feature_list = list(Xfull.columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Linear regression\n",
    "lr_model = LinearRegression()\n",
    "# lr_model.fit(X_train, Y_train)\n",
    "\n",
    "# Random forest (1000 decision trees)\n",
    "rf_model = RandomForestRegressor(n_estimators = 1000, random_state = 42)\n",
    "# rf_model.fit(X_train, Y_train);\n",
    "\n",
    "# XGB regressor\n",
    "xgb_model = XGBRegressor()\n",
    "# xgb_model.fit(X_train, Y_train);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cross-validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean mae of -260.30 with a standard deviation of 0.95\n",
      "Mean r2 of 0.48 with a standard deviation of 0.00\n",
      "Mean rmse of -354.47 with a standard deviation of 1.37\n"
     ]
    }
   ],
   "source": [
    "def run_model_with_cv(model, metrics, cv):\n",
    "  \n",
    "    scores = cross_validate(model, Xfull, Yfull, cv=cv, scoring=metrics ,return_estimator=True)\n",
    "\n",
    "    mae_scores = scores['test_neg_mean_absolute_error']\n",
    "    r2_scores = scores['test_r2']\n",
    "    rmse_scores = scores['test_neg_root_mean_squared_error']\n",
    "\n",
    "    # Report the mean scores\n",
    "    print(\"Mean mae of %0.2f with a standard deviation of %0.2f\" % (mae_scores.mean(), mae_scores.std()))\n",
    "    print(\"Mean r2 of %0.2f with a standard deviation of %0.2f\" % (r2_scores.mean(), r2_scores.std()))\n",
    "    print(\"Mean rmse of %0.2f with a standard deviation of %0.2f\" % (rmse_scores.mean(), rmse_scores.std())) \n",
    "\n",
    "metrics = ['neg_mean_absolute_error', 'r2', 'neg_root_mean_squared_error']\n",
    "cv = KFold(n_splits=10, random_state=1, shuffle=True)\n",
    "\n",
    "run_model_with_cv(lr_model, metrics, cv)   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "run_model_with_cv(rf_model, metrics, cv)   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Modelling - Linear regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "## Set up a dictionary containing the hyperparameters we want to tune\n",
    "hyperparameters_rf = { 'randomforestregressor__max_features' : ['auto', 'sqrt', 'log2'],\n",
    "                  'randomforestregressor__max_depth': [None, 5, 3, 1],\n",
    "                     'randomforestregressor__n_estimators': [50, 100, 150, 200, 250, 300, 350]}\n",
    "# Set up the pipeline containing the scalers\n",
    "pipeline_rf = make_pipeline(MinMaxScaler(feature_range = (0,1)), \n",
    "                         RandomForestRegressor())\n",
    "\n",
    "# Store the scores in a results dictionary (and print them)\n",
    "final_results = {}\n",
    "for model_values in [(pipeline_rf,  hyperparameters_rf,  'RandomForest')]:\n",
    "    \n",
    "    clf = GridSearchCV(model_values[0], model_values[1], \n",
    "                       cv = 10, # positive intiger means k-fold (e.g. 10-fold)\n",
    "                       #scoring  = 'neg_mean_squared_error', # MSE to calculate score\n",
    "                       scoring  = 'r2', # MSE to calculate score\n",
    "                       n_jobs=multiprocessing.cpu_count()) # Run on multiple cores\n",
    "    clf.fit(X_test, Y_test)\n",
    "    name = model_values[2]\n",
    "    final_results[name] = clf\n",
    "    print (\"Hyperparameter results for {}\".format(name))\n",
    "    print (\"\\tBest Score: {}\".format(clf.best_score_))\n",
    "    print (\"\\tBest params: {}\".format(clf.best_params_))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean mae of -130.77 with a standard deviation of 1.30\n",
      "Mean r2 of 0.75 with a standard deviation of 0.00\n",
      "Mean rmse of -243.62 with a standard deviation of 1.97\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import cross_validate, cross_val_score\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Find the best model  \n",
    "Use k-fold cross validation to evaluate a range of regression algorithms on the training data. Use a pipeline for evaluation which first scales the (weather) data. Print the results and assess which models perform best.\n",
    "\n",
    "The following models were trialled:\n",
    "\n",
    "* Decision Tree\n",
    "* Random Forest\n",
    "* Extra Trees\n",
    "* Dummy Regressor\n",
    "* Elastic Net CV\n",
    "* Passive Aggressive\n",
    "* RANSAC\n",
    "* SGD\n",
    "* TheilSen (dropped in code below because it takes too long)\n",
    "* K Neighbours\n",
    "* LinearRegression\n",
    "* XGBoost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Define a list of all the models to use\n",
    "# Models = {'LinearRegression': LinearRegression,'DecisionTree' : DecisionTreeRegressor,\n",
    "#           'RandomForest': RandomForestRegressor, 'ExtraTrees' : ExtraTreesRegressor,\n",
    "#           'DummyRegressor' :DummyRegressor, 'ElasticNetCV' : ElasticNetCV, \n",
    "#           'PassiveAggressive' : PassiveAggressiveRegressor, #RANSAC': RANSACRegressor, # This one is terrible too\n",
    "#           'SGD': SGDRegressor, #'TheilSen': TheilSenRegressor, # Drop this - it isn't great and takes too long\n",
    "#           'KN': KNeighborsRegressor}#, 'XGBoost': xgb.XGBRegressor}\n",
    " \n",
    "# # Now just run each model, but do this in multiple processes simultaneously to save time    \n",
    "# # Now call that function simultaneously for each model\n",
    "# p = Pool(processes=None) # A pool of processes (one for each core)\n",
    "# results = p.map(run_model, [(name, model_type) for name, model_type in Models.items()])\n",
    "\n",
    "# # Sort the results by median mse (that's item 5 in the tuple)\n",
    "# results.sort(key=lambda x: x[5], reverse=True)\n",
    "\n",
    "# # Put the results in a nice dictionary and print them\n",
    "# results_dict = {}\n",
    "# txt = \"<table><thead><td>Name</td><td>Median R2</td><td>Median MSE</td><td>runtime (sec)</td></thead>\"\n",
    "# for name, model, all_r2, r2, all_mse, mse, runtime in results:\n",
    "#     txt += \"<tr><td>{}</td><td>{}</td><td>{}</td><td>{}</td></tr>\".format(name, r2, mse, runtime)\n",
    "#     results_dict[name] = (model, all_r2, r2, all_mse, mse, runtime)\n",
    "# txt += \"</table>\"\n",
    "# display(HTML(txt)) # print as html\n",
    "\n",
    "# min_mse = min([mse for (name, model, all_r2, r2, all_mse, mse, runtime) in results])\n",
    "               \n",
    "# x =  [ name for (name, model, all_r2, r2, all_mse, mse, runtime) in results]\n",
    "# y1 = [ mse-min_mse   for (name, model, all_r2, r2, all_mse, mse, runtime) in results]\n",
    "# y2 = [ r2 if r2 > 0 else 0 for (name, model, all_r2, r2, all_mse, mse, runtime) in results]\n",
    "\n",
    "# fig, (ax1,ax2) = plt.subplots(1, 2, figsize=(15, 7))\n",
    "\n",
    "# ax1.set_title(\"MSE\")\n",
    "# #ax1.invert_yaxis()\n",
    "# ax1.bar(range(len(x)), y1)\n",
    "# ax1.set_xticks(range(len(x)))\n",
    "# ax1.set_xticklabels(x, rotation=90)\n",
    "# ax1.set_ylim([27000000000, 29000000000])\n",
    "\n",
    "# ax2.set_title(\"R^2\")\n",
    "# ax2.bar(range(len(x)), y2)\n",
    "# ax2.set_xticks(range(len(x)))\n",
    "# ax2.set_xticklabels(x, rotation=90)\n",
    "\n",
    "# plt.show()\n",
    "\n",
    "# #del x,y1, y2\n",
    "\n",
    "# ## Set up a dictionary containing the hyperparameters we want to tune\n",
    "# hyperparameters_rf = { 'randomforestregressor__max_features' : ['auto', 'sqrt', 'log2'],\n",
    "#                   'randomforestregressor__max_depth': [None, 5, 3, 1]}\n",
    "# # hyperparameters_xgb = {'xgbregressor__max_depth': range(1, 11, 2),\n",
    "# #                    'xgbregressor__n_estimators' : range(50, 400, 50),\n",
    "# #                    'xgbregressor__learning_rate' : [0.0001, 0.001, 0.01, 0.1, 0.2, 0.3]}\n",
    "# hyperparameters_lr = {}\n",
    "\n",
    "# # Set up the pipeline containing the scalers\n",
    "# pipeline_rf = make_pipeline(MinMaxScaler(feature_range = (0,1)), \n",
    "#                          RandomForestRegressor(n_estimators=100))\n",
    "# # pipeline_xgb = make_pipeline(MinMaxScaler(feature_range = (0,1)),\n",
    "# #                          xgb.XGBRegressor(n_estimators=100))\n",
    "# pipeline_lr = make_pipeline(MinMaxScaler(feature_range = (0,1)),\n",
    "#                          LinearRegression())\n",
    "\n",
    "# # Store the scores in a results dictionary (and print them)\n",
    "# final_results = {}\n",
    "# for model_values in [(pipeline_rf,  hyperparameters_rf,  'RandomForest'),\n",
    "# #                      (pipeline_xgb, hyperparameters_xgb, 'XGBoost'),\n",
    "#                      (pipeline_lr,  hyperparameters_lr,  'LinearRegression')]:\n",
    "    \n",
    "#     clf = GridSearchCV(model_values[0], model_values[1], \n",
    "#                        #cv = None, # Cross-validation method. None means default (3-fold)\n",
    "#                        cv = 10, # positive intiger means k-fold (e.g. 10-fold)\n",
    "#                        #scoring  = 'neg_mean_squared_error', # MSE to calculate score\n",
    "#                        scoring  = 'r2', # MSE to calculate score\n",
    "#                        n_jobs=multiprocessing.cpu_count()) # Run on multiple cores\n",
    "    \n",
    "#     #clf = GridSearchCV(model_values[0], model_values[1], cv = 10, scoring  = 'r2')\n",
    "#     clf.fit(X_validate, Y_validate)\n",
    "#     name = model_values[2]\n",
    "#     final_results[name] = clf\n",
    "#     print (\"Hyperparameter results for {}\".format(name))\n",
    "#     print (\"\\tBest Score: {}\".format(clf.best_score_))\n",
    "#     print (\"\\tBest params: {}\".format(clf.best_params_))"
   ]
  }
 ],
 "metadata": {
  "hide_input": false,
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
