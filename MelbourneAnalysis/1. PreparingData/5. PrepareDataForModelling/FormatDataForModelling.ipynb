{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5c55f228",
   "metadata": {},
   "source": [
    "# Prepare data in the format for modelling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "614fa147",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from warnings import simplefilter\n",
    "import sys\n",
    "simplefilter(action='ignore', category=FutureWarning)\n",
    "import geopy.distance\n",
    "\n",
    "def add_sin_and_cos_features(df, column_to_transform):\n",
    "    df['Sin_{}'.format(column_to_transform)] = np.sin(2 * np.pi * df[column_to_transform] / max(df[column_to_transform])) \n",
    "    df['Cos_{}'.format(column_to_transform)] = np.cos(2 * np.pi * df[column_to_transform] / max(df[column_to_transform]))\n",
    "    return df\n",
    "\n",
    "sys.path.append('../../3. Modelling')\n",
    "from Functions import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "006ffe58",
   "metadata": {},
   "outputs": [],
   "source": [
    "buffer_size_m = 400"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ef86b98",
   "metadata": {},
   "source": [
    "### Prepare footfall data\n",
    "<u> Including removing outliers:</u>  \n",
    "The model should predict normal footfall. Therefore any days that have extremely high or low footfall should be taken out of the training data. We don't actually want the model to try to predict footfall on unusual days, because the things that make the day unusual (like errors in the camera counters, or the presence of special events) are not captured in the input data.\n",
    "\n",
    "Outliers are detected using the Median Absolute Deviation method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "id": "574774e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read in data\n",
    "sensors = pd.read_csv('../../Cleaned_data/SensorData/allsensors.csv')\n",
    "# Create month as number not string\n",
    "sensors['datetime'] =pd.to_datetime(sensors['datetime'], format = '%Y-%m-%d %H:%M:%S')#dayfirst = False)\n",
    "# Keep only data from 2011 onwards\n",
    "sensors= sensors[sensors['year']>2010]\n",
    "# # Create a categrorial variable defining the time of day\n",
    "sensors['time_of_day'] = sensors.apply (lambda row: label_hours(row), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96ef8e05",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove outliers\n",
    "sensors, outliers = remove_outliers(sensors)\n",
    "# Drop unneeded columns\n",
    "sensors=sensors.drop(['Latitude', 'Longitude', 'location', 'mdate'], axis=1)\n",
    "# Check the data\n",
    "sensors.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0eac6c78",
   "metadata": {},
   "source": [
    "### Inspect outliers which have been removed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "id": "acdc79d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Look at outliers from particular day (which have reason to believe might have incldued outliers)\n",
    "# t = [pd.to_datetime('2020-03-07 13:00:00')]\n",
    "# this_day = outliers[outliers['datetime'].isin(t)]#[2255323]['datetime']\n",
    "# # Look at biggest outliers\n",
    "# outliers.nlargest(10, 'hourly_counts')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "681396e8",
   "metadata": {},
   "source": [
    "### Inspecting sensor which has a lot of the outliers removed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5bf7993",
   "metadata": {},
   "outputs": [],
   "source": [
    "# sensor_57 = sensors[sensors['sensor_id']==57]\n",
    "# # Filter by a single date\n",
    "# sensor_57_oneday = sensor_57[sensor_57['datetime'].dt.strftime('%Y-%m-%d') == \"2019-11-15\"]\n",
    "# sensor_57_oneday.index=sensor_57_oneday['time']\n",
    "# fig, ax = plt.subplots(figsize = (5,4), sharex = True)\n",
    "# fig = sensor_57_oneday['hourly_counts'].plot(ax=ax, color='darkred', linewidth=2, legend =True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79d41a9b",
   "metadata": {},
   "source": [
    "## add date variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "622560dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "sensors['day_of_month_num'] = sensors['datetime'].dt.day\n",
    "sensors['weekday_num'] = sensors['datetime'].dt.weekday +1\n",
    "sensors['month_num'] = sensors['datetime'].dt.month"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64a9efa4",
   "metadata": {},
   "source": [
    "### Join public holiday and weather data to sensor data (WHY DO NUMBER OF ROWS INCREASE SLIGHTLY?)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d08bfeac",
   "metadata": {},
   "outputs": [],
   "source": [
    "sensors_with_features = sensors.merge(pd.read_csv('../../Cleaned_data/WeatherData/weather_data_allyears.csv', parse_dates=['datetime']), on='datetime', how='left')\n",
    "sensors_with_features = sensors_with_features.merge(pd.read_csv('../../Cleaned_data/WeatherData/DailyRainfallData.csv', parse_dates=['datetime']), on='datetime', how='left')\n",
    "sensors_with_features = sensors_with_features.merge(pd.read_csv('../../Cleaned_data/HolidaysData/publicholidays.csv', parse_dates=['datetime']),how='left', on='datetime')\n",
    "sensors_with_features = sensors_with_features.merge(pd.read_csv('../../Cleaned_data/HolidaysData/schoolholidays.csv', parse_dates=['datetime']),how='left', on='datetime')\n",
    "sensors_with_features = sensors_with_features.merge(pd.read_csv('../../Cleaned_data/SpatialFeatures/sensors_betweenness.csv'),how='left', on='sensor_id')\n",
    "sensors_with_features = sensors_with_features.merge(pd.read_csv('../../Cleaned_data/FeaturesNearSensors/num_features_near_sensors_{}.csv'.format(buffer_size_m), index_col=0) ,how='left', on='sensor_id')\n",
    "sensors_with_features = sensors_with_features.merge(pd.read_csv('../../Cleaned_data/FeaturesNearSensors/feature_subtypes_near_sensors_{}.csv'.format(buffer_size_m), index_col=0) ,how='left', on='sensor_id')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f90f9d14",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Length is now:  4129082\n"
     ]
    }
   ],
   "source": [
    "print(\"Length is now: \" , len(sensors_with_features))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24a4ea08",
   "metadata": {},
   "source": [
    "### Add average number of floors of building in vicinity, for correct year "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31b6f9b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "sensors_with_features['avg_n_floors'] = sensors_with_features.apply (lambda row: select_n_floors(row), axis=1)\n",
    "sensors_with_features = sensors_with_features[sensors_with_features.columns.drop(list(sensors_with_features.filter(regex='avg_n_floors_')))]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c06562c2",
   "metadata": {},
   "source": [
    "### Add buildings (correctly for the year the data relates to)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09781314",
   "metadata": {},
   "outputs": [],
   "source": [
    "# FOR SENSORS WITH FEATURES\n",
    "sensors_with_features['buildings'] = sensors_with_features.apply (lambda row: select_buildings(row), axis=1).copy()\n",
    "sensors_with_features= sensors_with_features.drop(['buildings_2010', 'buildings_2011','buildings_2012', 'buildings_2013',\n",
    "                                                  'buildings_2014','buildings_2015','buildings_2016','buildings_2017',\n",
    "                                                  'buildings_2018', 'buildings_2019', 'buildings_2020'], axis =1)\n",
    "# FOR SENSORS WITH SUBFEATURES\n",
    "# Create a dataframe containing just the building subttypes for the year that this row refers to\n",
    "temp = pd.DataFrame(None)\n",
    "# For each year, get the data for just that year\n",
    "for year in range(2011,2022+1):   \n",
    "    # Get just footfall data for this year\n",
    "    this_year = sensors_with_features[sensors_with_features['year'] == year]\n",
    "    # If year is over 2020 then set the year to 2020 for the purposes of selecting the building data \n",
    "    if year > 2020:\n",
    "        year = 2020\n",
    "    # Get just the building columns for this year\n",
    "    buildings_this_yr = this_year.filter(like='{}'.format(year))\n",
    "    # Drop all the building subtype columns from the row (and the bikes) \n",
    "    this_year = this_year[this_year.columns.drop(list(this_year.filter(regex='bikes|buildings_')))]\n",
    "     # Join the row without any buildings, back to this row \n",
    "    this_year = pd.concat([this_year, buildings_this_yr], axis=1)\n",
    "    # Rename -- ??\n",
    "    this_year.columns = this_year.columns.str.replace(r'_{}'.format(year), '')\n",
    "    # Join to dataframe which will store data for all years eventually\n",
    "    temp = temp.append(this_year)\n",
    "sensors_with_features = temp.copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55504128",
   "metadata": {},
   "source": [
    "### Add dummy variables for calendar variables \n",
    "(Not doing this anymore as creating cos and sin variables)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e408c351",
   "metadata": {},
   "outputs": [],
   "source": [
    "# sensors_with_features=convert_df_variables_to_dummy(sensors_with_features, ['day', 'month', 'year', 'time'])\n",
    "# sensors_with_features=convert_df_variables_to_dummy(sensors_with_features, ['time_of_day'])\n",
    "# sensors_with_features = sensors_with_features.drop(['datetime'],axis=1)\n",
    "\n",
    "### Date based variables: Option 2 - Create Dummy Variables\n",
    "for date_col in ['day', 'month',]:\n",
    "    date_col_dummy =  pd.get_dummies(sensors_with_features[date_col], drop_first = True)\n",
    "    if date_col =='month':\n",
    "        date_col_dummy.columns= prepend(date_col_dummy.columns.values, 'month_')\n",
    "    # if date_col =='year':\n",
    "        # date_col_dummy.columns= prepend(date_col_dummy.columns.values, 'year_')\n",
    "    sensors_with_features = pd.concat([sensors_with_features, date_col_dummy],axis=1)\n",
    "    del sensors_with_features[date_col]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1bc43ef",
   "metadata": {},
   "source": [
    "### Create sin/cos variables to represent cyclical features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51a55eea",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Time refers to the hour of the day 0-23 (makes sense that this is cyclical - relationship between 23 and 0)\n",
    "sensors_with_features = add_sin_and_cos_features(sensors_with_features, 'time')\n",
    "# Month number from 1-12 (makes sense that this is cyclical - relationship between 12 and 1)\n",
    "sensors_with_features = add_sin_and_cos_features(sensors_with_features, 'month_num')\n",
    "# Weekday number from 1-7 (maybe doesnt make sense that this is cyclical - doesn't really follow a logical pattern?\n",
    "sensors_with_features = add_sin_and_cos_features(sensors_with_features, 'weekday_num')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "431335c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "ax = sensors_with_features.plot.scatter('Sin_weekday_num', 'Cos_weekday_num').set_aspect('equal')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b536828",
   "metadata": {},
   "source": [
    "### Replace NaNs with 0s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9742e37a",
   "metadata": {},
   "outputs": [],
   "source": [
    "sensors_with_features= sensors_with_features.fillna(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "704467ef",
   "metadata": {},
   "source": [
    "## Create aggregated data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0cef5619",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ### Filter to include just sensors which we know have quite complete data \n",
    "# data = sensors_with_features[sensors_with_features['sensor_id'].isin([2,6,9,10,14,18])]\n",
    "# data.reset_index(inplace=True, drop = True)\n",
    "\n",
    "# # Get just this hourly counts\n",
    "# hourly_counts = data[['datetime', \"hourly_counts\"]]\n",
    "# # Get the sum of all values for one hour\n",
    "# summed_hourly_counts = hourly_counts.groupby(\"datetime\").sum()\n",
    "# # reset index (for joining)\n",
    "# summed_hourly_counts.reset_index(inplace = True, drop = True)\n",
    "\n",
    "# # Get just the features\n",
    "# features = data[['datetime','Temp', 'Humidity', 'Pressure', 'Rain', 'WindSpeed',\n",
    "#        'public_holiday','school_holiday','Sin_time', 'Cos_time', 'Sin_month_num', 'Cos_month_num', 'Sin_weekday_num', 'Cos_weekday_num']]\n",
    "# # Keep only one version of each row (should all be the same)\n",
    "# features = features.drop_duplicates(keep='last')\n",
    "# # reset index (for joining)\n",
    "# features.reset_index(inplace = True, drop = True)\n",
    "\n",
    "# # Join features to aggregated count values\n",
    "# aggregated = pd.concat([summed_hourly_counts, features], axis=1, join='inner')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a279d2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# for column in sensors_with_features.columns:\n",
    "#     print(column, np.nanmax(sensors_with_features[column]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc0536d9",
   "metadata": {},
   "source": [
    "### Remove time variables no longer required"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97e941fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# sensors_with_features = sensors_with_features.drop([ 'Sin_month_num', 'Cos_month_num', 'Sin_weekday_num',\n",
    "#        'Cos_weekday_num',],axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9fea2ac",
   "metadata": {},
   "source": [
    "### Add distance from centre as a variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fcc6b531",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Read in Melbourne sensor location spatial data\n",
    "melbourne_sensors = pd.read_csv(\"../../Data/FootfallData/melbourne_locations.csv\")\n",
    "melbourne_sensors.rename(columns={'sensor_description': 'Name'}, inplace = True)\n",
    "melbourne_sensors = melbourne_sensors.drop_duplicates('sensor_id', keep='first')\n",
    "\n",
    "# Coordinates of 'centre' of CBD (done on google maps)\n",
    "coords_1 = (-37.812187461761596, 144.962265054567)\n",
    "distances =[]\n",
    "for row_number in range(0,len(melbourne_sensors)):\n",
    "    coords_2 = (melbourne_sensors['Latitude'][row_number], melbourne_sensors['Longitude'][row_number])\n",
    "    distances.append(geopy.distance.geodesic(coords_1, coords_2).km)\n",
    "    \n",
    "melbourne_sensors['distance_from_centre']=distances\n",
    "melbourne_sensors=melbourne_sensors[['sensor_id','distance_from_centre']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37c6c067",
   "metadata": {},
   "outputs": [],
   "source": [
    "sensors_with_features= pd.merge(sensors_with_features, melbourne_sensors, on=['sensor_id'], how='left')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "230ad911",
   "metadata": {},
   "source": [
    "### Save as csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ca71733",
   "metadata": {},
   "outputs": [],
   "source": [
    "sensors_with_features.to_csv(\"../../Cleaned_data/FormattedDataForModelling/formatted_data_for_modelling_allsensors_{}.csv\".format(buffer_size_m),\n",
    "                            index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f06a83e",
   "metadata": {},
   "outputs": [],
   "source": [
    "sensors_with_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f69f16f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def doubleMADsfromMedian(y,thresh=3.5):\n",
    "    \"\"\"Find outliers using the Median Average Distance.\n",
    "    \n",
    "    VALUE: return a list of true/false denoting whether the element in y is an outlier or not\n",
    "    \n",
    "    PARAMETERS:\n",
    "      - y is a pandas Series, or something like that.\n",
    "    \n",
    "    warning: this function does not check for NAs\n",
    "    nor does it address issues when \n",
    "    more than 50% of your data have identical values\n",
    "    \"\"\"\n",
    "    # Calculate the upper and lower limits\n",
    "    m = np.median(y) # The median\n",
    "    abs_dev = np.abs(y - m) # The absolute difference between each y and the median\n",
    "    # The upper and lower limits are the median of the difference\n",
    "    # of each data point from the median of the data\n",
    "    left_mad = np.median(abs_dev[y <= m]) # The left limit (median of lower half)\n",
    "    right_mad = np.median(abs_dev[y >= m]) # The right limit (median of upper half)\n",
    "    \n",
    "    # Now create an array where each value has left_mad if it is in the lower half of the data,\n",
    "    # or right_mad if it is in the upper half\n",
    "    y_mad = left_mad * np.ones(len(y)) # Initially every value is 'left_mad'\n",
    "    y_mad[y > m] = right_mad # Now larger values are right_mad\n",
    "\n",
    "    # Calculate the z scores for each element\n",
    "    modified_z_score = 0.6745 * abs_dev / y_mad\n",
    "    modified_z_score[y == m] = 0\n",
    "    \n",
    "    # Return boolean list showing whether each y is an outlier\n",
    "    return modified_z_score > thresh\n",
    "\n",
    "# Make a list of true/false for whether the footfall is an outlier\n",
    "# no_outliers = pd.DataFrame(doubleMADsfromMedian(sensors_with_features['hourly_counts']))\n",
    "# no_outliers.columns = ['outlier'] # Rename the column to 'outlier'\n",
    "\n",
    "# # Join to the original footfall data to the list of outliers, then select a few useful columns\n",
    "join = pd.concat([sensors_with_features, no_outliers], axis = 1)\n",
    "# join = pd.DataFrame(join, columns = ['Day_yr', 'outlier', 'InCount'])\n",
    "join\n",
    "# # Choose just the outliers\n",
    "outliers = join[join['outlier'] == True]\n",
    "outliers_list = list(outliers['datitime']) # A list of the days that are outliers\n",
    "outliers_list\n",
    "# # Now remove all outliers from the original data\n",
    "# df = original.loc[~original['Day_yr'].isin(outliers_list)]\n",
    "# df = df.reset_index(drop = True)\n",
    "\n",
    "# # Check that the lengths all make sense\n",
    "# assert(len(df) == len(original)-len(outliers_list))\n",
    "\n",
    "# print(\"I found {} outliers from {} days in total. Removing them leaves us with {} events\".format(\\\n",
    "#     len(outliers_list), len(join), len(df) ) )"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
